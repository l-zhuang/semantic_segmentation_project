{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENet initial block\n",
    "class InitialBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch\n",
    "        self.main_branch = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels - 3,\n",
    "            kernel_size = 3,\n",
    "            stride = 2,\n",
    "            padding = 1,\n",
    "            bias=bias)\n",
    "        # Extension branch\n",
    "        self.ext_branch = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Initialize batch normalization to be used after concatenation\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        # PReLU layer to apply after concatenating the branches\n",
    "        self.out_activation = activation()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        main = self.main_branch(x)\n",
    "        ext = self.ext_branch(x)\n",
    "        # Concatenate branches\n",
    "        out = torch.cat((main, ext), 1)\n",
    "        # Apply batch normalization\n",
    "        out = self.batch_norm(out)\n",
    "        return self.out_activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENet regular bottleneck module, \n",
    "# 3 options: [1]:regular; [2]: changed dilated number; [3]: asymmetric\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 internal_ratio=4,\n",
    "                 kernel_size=3,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 asymmetric=False,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        if internal_ratio <= 1 or internal_ratio > channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}.\"\n",
    "                               .format(channels, internal_ratio))\n",
    "        \n",
    "        internal_channels = channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # 1x1 projection convolution\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                internal_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), \n",
    "                nn.BatchNorm2d(internal_channels), \n",
    "                activation())\n",
    "        \n",
    "        # asymmetric convolution or regular, dilated or full convolution with 3 × 3 filters\n",
    "        \n",
    "        if asymmetric:\n",
    "            # asymmetric convolution has a sequence of 5 × 1 and 1 × 5 convolutions\n",
    "            self.ext_conv2 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=(5,1),\n",
    "                    stride=1,\n",
    "                    padding=(padding, 0),\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), \n",
    "                    nn.BatchNorm2d(internal_channels), \n",
    "                    activation(),\n",
    "\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=(1,5),\n",
    "                    stride=1,\n",
    "                    padding=(0, padding),\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), \n",
    "                    nn.BatchNorm2d(internal_channels), \n",
    "                    activation()) \n",
    "        else:\n",
    "            self.ext_conv2= nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size = kernel_size,\n",
    "                    stride = 1,\n",
    "                    padding = padding,\n",
    "                    dilation = dilation,\n",
    "                    dropout_prob=0,\n",
    "                    bias=bias), \n",
    "                    nn.BatchNorm2d(internal_channels), \n",
    "                    activation())\n",
    "\n",
    "        self.ext_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                internal_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), \n",
    "                nn.BatchNorm2d(internal_channels), \n",
    "                activation())\n",
    "        \n",
    "        # For the regularizer, we use Spatial Dropout\n",
    "        #with p = 0.01 before bottleneck2.0, and p = 0.1 afterwards\n",
    "        self.ext_reg = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after adding the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch with no operations\n",
    "        main = x\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_conv3(ext)\n",
    "        ext = self.ext_reg(ext)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENet downsampling bottleneck module\n",
    "class Downsampling(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 internal_ratio=4,\n",
    "                 return_indices=False,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}.\"\n",
    "                               .format(in_channels, internal_ratio))\n",
    "        \n",
    "        internal_channels = in_channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # If the bottleneck is downsampling, a max pooling layer is added to the main branch\n",
    "        self.main_conv1 = nn.MaxPool2d(2,stride=2,return_indices=return_indices)\n",
    "\n",
    "        # 1x1 projection convolution\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                internal_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                bias=bias), \n",
    "                nn.BatchNorm2d(internal_channels), \n",
    "                activation())\n",
    "        \n",
    "        self.ext_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                internal_channels,\n",
    "                kernel_size = 3,\n",
    "                stride = 1,\n",
    "                bias=bias), \n",
    "                nn.BatchNorm2d(internal_channels), \n",
    "                activation())\n",
    "\n",
    "        self.ext_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), \n",
    "                nn.BatchNorm2d(internal_channels), \n",
    "                activation())\n",
    "        \n",
    "        # For the regularizer, we use Spatial Dropout\n",
    "        #with p = 0.01 before bottleneck2.0, and p = 0.1 afterwards\n",
    "        self.ext_reg = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after adding the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch with Maxpooling and padding\n",
    "        if self.return_indices:\n",
    "            main, max_indices = self.main_conv1(x)\n",
    "        else:\n",
    "            main = self.main_conv1(x)\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_conv3(ext)\n",
    "        ext = self.ext_reg(ext)\n",
    "\n",
    "       # Main branch channel padding\n",
    "        n, ch_ext, h, w = ext.size()\n",
    "        ch_main = main.size()[1]\n",
    "        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n",
    "\n",
    "        # Before concatenating, check if main is on the CPU or GPU and\n",
    "        # convert padding accordingly\n",
    "        if main.is_cuda:\n",
    "            padding = padding.cuda()\n",
    "\n",
    "        # Concatenate the Maxpooling layer and Padding layer\n",
    "        main = torch.cat((main, padding), 1)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out), max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENet upsampling bottleneck module\n",
    "class Upsampling(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 internal_ratio=4,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}.\"\n",
    "                               .format(in_channels, internal_ratio))\n",
    "        \n",
    "        internal_channels = in_channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # If the bottleneck is downsampling, a max pooling layer is added to the main branch\n",
    "        self.main_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, \n",
    "                      out_channels, \n",
    "                      kernel_size=1, \n",
    "                      bias=bias),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, \n",
    "                      internal_channels, \n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=bias),\n",
    "\n",
    "            nn.BatchNorm2d(internal_channels),\n",
    "            activation())\n",
    "        \n",
    "        # transpose layer\n",
    "\n",
    "        self.ext_trans1 = nn.ConvTranspose2d(internal_channels, \n",
    "                               internal_channels, \n",
    "                               kernel_size=2,\n",
    "                               stride=2, \n",
    "                               bias=bias),\n",
    "\n",
    "        self.ext_trans1_bnorm = nn.BatchNorm2d(internal_channels)\n",
    "        self.ext_tconv1_activation = activation()\n",
    "\n",
    "        self.ext_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(internal_channels, \n",
    "                      out_channels, \n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=bias),\n",
    "\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x, max_indices, output_size):\n",
    "        # Main branch shortcut\n",
    "        main = self.main_conv1(x)\n",
    "        main = self.main_unpool1(main, max_indices, output_size=output_size)\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_trans1(ext, output_size=output_size)\n",
    "        ext = self.ext_trans1_bnorm(ext)\n",
    "        ext = self.ext_trans1_activation(ext)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_regul(ext)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, encoder_relu=False, decoder_relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial_block = InitialBlock(3, 16, relu=encoder_relu)\n",
    "\n",
    "        # Stage 1 - Encoder\n",
    "        self.downsample1_0 = Downsampling(16,64,return_indices=True,dropout_prob=0.01,relu=encoder_relu)\n",
    "        \n",
    "        self.regular1_1 = Bottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "\n",
    "        self.regular1_2 = Bottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "\n",
    "        self.regular1_3 = Bottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "\n",
    "        self.regular1_4 = Bottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "\n",
    "        # Stage 2 - Encoder\n",
    "        self.downsample2_0 = Downsampling(64,128,return_indices=True,dropout_prob=0.1,relu=encoder_relu)\n",
    "\n",
    "        self.regular2_1 = Bottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        self.dilated2_2 = Bottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        self.asymmetric2_3 = Bottleneck(128,kernel_size=5, padding=2,asymmetric=True,dropout_prob=0.1,relu=encoder_relu)\n",
    "\n",
    "        self.dilated2_4 = Bottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        \n",
    "        self.regular2_5 = Bottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        \n",
    "        self.dilated2_6 = Bottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        \n",
    "        self.asymmetric2_7 = Bottleneck(128,kernel_size=5, padding=2,asymmetric=True,dropout_prob=0.1,relu=encoder_relu)\n",
    "\n",
    "        self.dilated2_8 = Bottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # Stage 3 - Encoder\n",
    "        self.regular3_0 = Bottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        self.dilated3_1 = Bottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        self.asymmetric3_2 = Bottleneck(128,kernel_size=5, padding=2,asymmetric=True,dropout_prob=0.1,relu=encoder_relu)\n",
    "\n",
    "        self.dilated3_3 = Bottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        \n",
    "        self.regular3_4 = Bottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        \n",
    "        self.dilated3_5 = Bottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        \n",
    "        self.asymmetric3_6 = Bottleneck(128,kernel_size=5, padding=2,asymmetric=True,dropout_prob=0.1,relu=encoder_relu)\n",
    "\n",
    "        self.dilated3_7 = Bottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # Stage 4 - Decoder\n",
    "        self.upsample4_0 = Upsampling(128, 64, dropout_prob=0.1, relu=decoder_relu)\n",
    "\n",
    "        self.regular4_1 = Bottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        self.regular4_2 = Bottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # Stage 5 - Decoder\n",
    "        self.upsample5_0 = Upsampling(64, 16, dropout_prob=0.1, relu=decoder_relu)\n",
    "\n",
    "        self.regular5_1 = Bottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        self.transposed_conv = nn.ConvTranspose2d(16,num_classes,kernel_size=3,stride=2,padding=1,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        input_size = x.size()\n",
    "        x = self.initial_block(x)\n",
    "\n",
    "        # Stage 1 - Encoder\n",
    "        stage1_input_size = x.size()\n",
    "        x, max_indices1_0 = self.downsample1_0(x)\n",
    "        x = self.regular1_1(x)\n",
    "        x = self.regular1_2(x)\n",
    "        x = self.regular1_3(x)\n",
    "        x = self.regular1_4(x)\n",
    "\n",
    "        # Stage 2 - Encoder\n",
    "        stage2_input_size = x.size()\n",
    "        x, max_indices2_0 = self.downsample2_0(x)\n",
    "        x = self.regular2_1(x)\n",
    "        x = self.dilated2_2(x)\n",
    "        x = self.asymmetric2_3(x)\n",
    "        x = self.dilated2_4(x)\n",
    "        x = self.regular2_5(x)\n",
    "        x = self.dilated2_6(x)\n",
    "        x = self.asymmetric2_7(x)\n",
    "        x = self.dilated2_8(x)\n",
    "\n",
    "        # Stage 3 - Encoder\n",
    "        x = self.regular3_0(x)\n",
    "        x = self.dilated3_1(x)\n",
    "        x = self.asymmetric3_2(x)\n",
    "        x = self.dilated3_3(x)\n",
    "        x = self.regular3_4(x)\n",
    "        x = self.dilated3_5(x)\n",
    "        x = self.asymmetric3_6(x)\n",
    "        x = self.dilated3_7(x)\n",
    "\n",
    "        # Stage 4 - Decoder\n",
    "        x = self.upsample4_0(x, max_indices2_0, output_size=stage2_input_size)\n",
    "        x = self.regular4_1(x)\n",
    "        x = self.regular4_2(x)\n",
    "\n",
    "        # Stage 5 - Decoder\n",
    "        x = self.upsample5_0(x, max_indices1_0, output_size=stage1_input_size)\n",
    "        x = self.regular5_1(x)\n",
    "        x = self.transposed_conv(x, output_size=input_size)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
